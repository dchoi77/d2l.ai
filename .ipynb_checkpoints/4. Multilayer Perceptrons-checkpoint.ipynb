{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.5. Weight Decay\n",
    "\n",
    "## 4.5.4. Concise Implementation\n",
    "\n",
    "Here I added two hidden layers. In this case, `net.losses` is a list of two tensors. Thus use `sum(net.losses)` when computing a batch loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2l import tensorflow as d2l\n",
    "import tensorflow as tf\n",
    "\n",
    "n_train, n_test, num_inputs, batch_size = 20, 100, 200, 5\n",
    "true_w, true_b = tf.ones((num_inputs, 1)) * 0.01, 0.05\n",
    "train_data = d2l.synthetic_data(true_w, true_b, n_train)\n",
    "train_iter = d2l.load_array(train_data, batch_size)\n",
    "test_data = d2l.synthetic_data(true_w, true_b, n_test)\n",
    "test_iter = d2l.load_array(test_data, batch_size, is_train=False)\n",
    "\n",
    "wd = 0.5            # wd is lambda\n",
    "num_inputs = 200    # num_inputs is d (feature size)\n",
    "num_epochs, lr = 100, 0.003\n",
    "\n",
    "net = tf.keras.models.Sequential()\n",
    "net.add(tf.keras.layers.Dense(10, kernel_regularizer=tf.keras.regularizers.l2(wd)))\n",
    "net.add(tf.keras.layers.Dense(1, kernel_regularizer=tf.keras.regularizers.l2(wd)))\n",
    "net.build(input_shape=(1, num_inputs))\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "trainer = tf.keras.optimizers.SGD(learning_rate=lr)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in train_iter:\n",
    "        with tf.GradientTape() as tape:\n",
    "            # `tf.keras` requires retrieving and adding the losses from\n",
    "            # layers manually for custom training loop.\n",
    "            l = loss(net(X), y) + sum(net.losses)\n",
    "        grads = tape.gradient(l, net.trainable_variables)\n",
    "        trainer.apply_gradients(zip(grads, net.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.6. Dropout\n",
    "\n",
    "Let $X$ be an input (minibatch) to a dropout layer $D$. \n",
    "\n",
    "If $h$ is a scalar entry in $X$, then the dropout layer makes $h$ either zero with probability $p$ or $h/(1-p)$ with probability $1-p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
